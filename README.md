# Code to understand and train your own GPT2 without much background

I started this blogpost under `yourGPT.ipynb` for people without little or no background in AI and/or software. It serves as an introduction to large language modelling (LLM). 

The repository also contains the code I used to train a slightly larger model on lighting.ai. However, the models are not very optimized and the intended purposes of them are largely educational.  

To get started, have a look at: https://colab.research.google.com/drive/1J9qJ-JArJSORVfOcppoD4cMC3u2I7aGZ?usp=sharing 
