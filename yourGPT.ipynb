{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# chatGPT explained"
      ],
      "metadata": {
        "id": "u6jALMKZsa-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this Colab notebook, I aim to bring a better understanding about how chatGPT at the core works to everyone. For this we are training your own super small GPT!"
      ],
      "metadata": {
        "id": "Zt_nAz3RtntG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're just looking for a short explanation on how chatGPT was somewhat trained and works:\n",
        "\n",
        "1. Collect a large amount of text data.\n",
        "\n",
        "2. Create a mapping from natural numbers to words, word-chunks, characters, bytes, etc. These are then called `tokens`.\n",
        "\n",
        "3. Assign them a randomly initialized vector (think back to high-school: a 2-dimensional vector starting on the standard x- and y-axis origin [0,0] can point in any direction, for instance [1,1] will point to the right and up). The vector is then adapted step by step in training.\n",
        "\n",
        "4. Define a transformer deep learning model based on the attention mechanism that allows vectors to incorporate knowledge from previous characters. (In `This is awesome`, the `i` has a different meaning in `This` and `is` depending on the context)\n",
        "\n",
        "5. Take your big dataset, transform it with your mapping from text to natural numbers into a sequence of numbers.\n",
        "\n",
        "6. Cut the number sequence into pieces and teach the model to predict the next number. Say our training text is four words: `\"This is an elephant\"`, we can assign them the numbers `0`, `1`, `2`, `3`. Then we simply teach the model to always predict the next number based on all previous numbers. So if the input (called \"context\") is `\"This is\"` or `[0, 1]`, the model should with high probability predict `[2]`. The input number sequence is first mapped to the 0th and 1st learnable vectors and then passed through the model for prediction. A model prediction is a probability distribution over all unique tokens in your dataset.\n",
        "\n",
        "7. If you think of the tokens as vectors that can be adapted, think of each dimension and the different multiplications happening in the model as knobs that have an effect on the final prediction. To actually `teach` the model, you simply check how wrong the prediction for the next character was and change all knobs slightly.\n",
        "\n",
        "8. Finally, you can pass some input text to the model and then keep predicting the distribution for the next character and pick the most probable one. There are also other variants to determine the next character. If you had a model trained on the entire internet and start with `chatGPT explained` you might even end up with this collab notebook! ðŸ˜‰\n",
        "\n",
        "\n",
        "Of course there is more to chatGPT. The model has thereby not yet learned that you interact with it in a `chat` fashion, etc. But this follows a similar regime by just `fine-tuning` the large model on chat-format text and giving the right input to the model when looking for a response to a user question. For instance you could create a dataset with multiple examples as follows:\n",
        "\n",
        "```\n",
        "System: You are a helpful AI assistant.\n",
        "User: What is the capital city of Switzerland?\n",
        "Assitant: While many people believe Zurich is the capital of\n",
        "    Switzerland, the capital city is Bern.\n",
        "User: Thanks. How many inhabitants does the country have?\n",
        "Assitant: The population of Switzerland is roughly 9 million.\n",
        "```\n",
        "\n",
        "Hence, if you type anything in chatGPT, they might send to the model something as follows:\n",
        "```\n",
        "System: You are a helpful AI assistant.\n",
        "User: {here_comes_your_question}\n",
        "Assitant:\n",
        "```\n",
        "Then, the model will start to generate the `Assistant's` answer!"
      ],
      "metadata": {
        "id": "nIaINk8rt7ar"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0-37wHbOVrt"
      },
      "source": [
        "\n",
        "### How are we doing this?\n",
        "\n",
        "In this notebook, we do the following steps to train our small GPT Model:\n",
        "1. Select some training data.\n",
        "2. Replicate (very small) GPT model architecture.\n",
        "3. Build training loop and train the model.\n",
        "4. Check what the model is able to come up with.\n",
        "\n",
        "For simplicity we align with [Andrej Karpathy's nanoGPT github repository](https://github.com/karpathy/nanoGPT/). This means we base the model on character predictions. Hence, we build a character-level encoder/decoder as tokenizer to map text to numbers that we can predict and the inverse. We also use the TinyShakespeare dataset that contains a collection of Shakespeare's work. Though feel free to upload your own text!\n",
        "\n",
        "I made the code in this notebook explicit such that anyone should be able to follow and understand variables and what is happening when we do run something."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### How do I run it?\n",
        "First we should connect to a GPU which makes deep learning much faster. For this, select `Runtime` in the file menu, click `Change runtime type` and select for instance the `T4 GPU`.\n",
        "\n",
        "Next, simply go read through the cells and press the `play` button on the left of every cell or when clicking on the first cell keep pressing `shift` + `enter`. **Make sure you run all cells in the order that they are.** You only need to run the `code` cells that have a grey background.\n",
        "**You could also first run all cells until `2.5 Generating text with yourGPT` before coming back and understanding them one-by-one**. This way, your GPT model trains while you read and learn."
      ],
      "metadata": {
        "id": "1rOTXX8-Spcu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dshZTSAAQ-Q5"
      },
      "source": [
        "# 1.&nbsp;Select and prepare data\n",
        "\n",
        "We select a simple text file containing many of Shakespeare's works called tinyshakespeare as dataset. If you would like to play with your own text file. Do not run the next cell and simply upload a file with your raw text called `input.txt` on the left sidebar to the folder icon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-L9WHe-OMQb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Get training data from github. If you want to use your own, upload an input.txt and do not run this cell!\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE8ta5GDQ6jy"
      },
      "source": [
        "Next, we load the dataset in python and split it into a training and validation dataset. The validation dataset is needed to check that the model does not simply memorize the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcGB0hs9P4b_"
      },
      "outputs": [],
      "source": [
        "with open(\"input.txt\") as f:\n",
        "  raw_data = f.read()\n",
        "\n",
        "train_data = raw_data[:int(0.9 * len(raw_data))]\n",
        "validation_data = raw_data[int(0.9 * len(raw_data)):]\n",
        "\n",
        "print(f\"We have a total number of characters in this dataset of: {len(raw_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd5D3OPRIq0"
      },
      "source": [
        "As mentioned, to train a neural network to predict text, the text needs to be in numeric form. There exist various so-called tokenizers to achieve this.\n",
        "\n",
        "A simply start is to take each character separately and encode it as a number. We check the the dataset for all unique characters and assign each character a number.\n",
        "\n",
        "As we then teach the model to predict a number that follows a new number sequence (the `generative` part of `generative pre-trained transformers`), we also need a number to character mapping, i.e., the inverse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krObUafLQHLO"
      },
      "outputs": [],
      "source": [
        "# Define the tokenizer with encoding and decoding methods\n",
        "all_unique_characters = set(train_data)\n",
        "\n",
        "# We add an <|UNKOWN|> token for characters not found in the training set\n",
        "all_unique_characters.add(\"<|UNKOWN|>\")\n",
        "\n",
        "n_unique_characters = len(all_unique_characters)\n",
        "print(f\"The unique characters in the dataset are as follows:\\n {''.join(sorted(all_unique_characters))}\\n\")\n",
        "\n",
        "\n",
        "# Now we need a mapping from character to number and vice versa.\n",
        "# Mappings are only one-way, hence, we need both directions\n",
        "character_to_number_mapping = {c: i for i, c in enumerate(sorted(all_unique_characters))}\n",
        "number_to_character_mapping = {i: c for c, i in character_to_number_mapping.items()}\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"The tokenizer is the 'object' that we can later use to go from text to numbers and the inverse.\n",
        "    \"\"\"\n",
        "    def __init__(self, c_2_n_mapping, n_2_c_mapping):\n",
        "        self.encoding_map = c_2_n_mapping\n",
        "        self.decoding_map = n_2_c_mapping\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.encoding_map.get(c, character_to_number_mapping[\"<|UNKOWN|>\"]) for c in text]\n",
        "\n",
        "    def decode(self, numbers):\n",
        "        return \"\".join([self.decoding_map[n] for n in numbers])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Finally, we create the tokenizer 'object' that can encode text and decode a sequence of numbers. Let's also briefly check how the tokenizer works, and that it works as expected by mapping characters to numbers and back."
      ],
      "metadata": {
        "id": "Xo_7HKpNgGv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(character_to_number_mapping, number_to_character_mapping)\n",
        "\n",
        "test_string = \"It's awesome to learn about chatGPT!\"\n",
        "encoded_test_string = tokenizer.encode(test_string)\n",
        "decoded_test_string = tokenizer.decode(encoded_test_string)\n",
        "\n",
        "print(f\"When encoding '{test_string}' with our tokenizer we get: {encoded_test_string}.\\n\")\n",
        "print(f\"Decoding the resulting sequence of numbers, we receive this: '{decoded_test_string}'.\")\n"
      ],
      "metadata": {
        "id": "PQrpBXZ5gHYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB_-6V9RWMp1"
      },
      "source": [
        "# 2.&nbsp;Generative Pretrained Transformer (GPT) Model Architecture\n",
        "\n",
        "Awesome, we already have a mapping from characters to numbers and the inverse!\n",
        "\n",
        "\n",
        "Now we need to define a deep learning model that we can teach to predict the next number based on a given sequence. For this we take the basic implementation of a Generative Pretrained Transformer (the `GPT` part in `chatGPT`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR6j6iFqdZJm"
      },
      "source": [
        "![GPT2 Architecture](https://raw.githubusercontent.com/serced/gpt2/main/assets/gpt2_model_architecture.png)\n",
        "\n",
        "*GPT2 Architecture: Taken from Steve D Yang et al. (2023) (https://pubs.acs.org/doi/10.1021/acs.iecr.3c01639)*\n",
        "\n",
        "\n",
        "\n",
        "As shown in the yellow part of the graphic, Generative Pretrained Transformers (GPT) are made up of several core modules.\n",
        "\n",
        "1. One of them is an embedding. It maps a character number to a vector consisting of multiple floating numbers (trivial, thus not shown on the image).\n",
        "\n",
        "2. The second one is a positional encoding. The transformer architecture does not have a notion of where the character is in the text, this is why we add pre-defined positional encoding e.g. based on a sine/cosine fromula such that it can understand where each token in the sequence is.\n",
        "\n",
        "3. Next we have the transformer block (orange background). This contains most of the deep learning part and can be repeated multiple times leading to more learnable parameters (you may have heard that e.g. Meta's Llama 3 model family having 8 or 70 billion learnable parameters). chatGPT has  ~1.8 trillion parameters as Nvidia's CEO Jensen Huang reveiled (see [Yahoo article about GPT-5](https://sg.news.yahoo.com/chatgpt-could-gpt-5-upgrade-175039034.html)). Though, not all of them are possibly used at the same time.\n",
        "\n",
        "4. In the end, we have a layer normalization and a linear layer (last two steps on the yellow background). The layer norm basically normalizes the output of the previous layer and the linear layer is projecting the input into the output space (what we predict). This is then normalized to predicting the probability of each unique token in the data. Over all possible tokens (characters in our case), this will then sum to 100%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQFZQg-4Q8Kt"
      },
      "source": [
        "## 2.1 Embedding and positional encoding\n",
        "\n",
        "Now we start using PyTorch. PyTorch is a framework that helps in training neural networks. As a first step, we are importing some components used later on. Using this framework, we can make use of `autograd`. This helps because we only need to implement the `forward pass`. The forward pass describes how the model forms a prediction. When we start training to adapt the `knobs`, there is a functionality by autograd that automatically does the `backward pass`. Essentially, this does minor changes to the learnable parameters (e.g. one single weight in a weight matrix can be 0.1234 and after a training step it could be 0.1233)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3rml_KxRA8M"
      },
      "outputs": [],
      "source": [
        "# Deep Learning framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Other utilities\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define some basic variables. We want each character to be represented as a 32-dimensional vector (```EMBEDDING_SIZE```). Moreover, we teach our model to learn over a maximum of 32 characters (```MAXIMUM_SEQUENCE_LENGTH```), which equates to roughly one sentence."
      ],
      "metadata": {
        "id": "Qe4SfUXPFSAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_SIZE = 32\n",
        "MAXIMUM_SEQUENCE_LENGTH = 32"
      ],
      "metadata": {
        "id": "YEUcmFq6FjGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3-9KhtKSzc"
      },
      "source": [
        "The original transformer paper uses a sinusodial positional encoding which is a\n",
        "hardcoded matrix that you simply add to the character vectors. It's based on a sine/cosine formula. Effectively, we add a small number to each character vector dimension to help the model to differentiate where in a sequence the token is and how tokens relate to each other.\n",
        "\n",
        "You could also add the actual integer position of the character to each dimension of its character vector. Say the character `s` in `this` is in the 4th position, we could add `4` to each dimension of the `s` vector representation. However, this will not work that well whereas the sine/cosine formula provides a  high-dimensional encoding that captures relative positional relationships, allowing the model to generalize better across different sequences and positional contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYZwz_OzRSZW"
      },
      "outputs": [],
      "source": [
        "def get_positional_encoding(max_seq_len, d_model):\n",
        "    pos_enc = np.zeros((max_seq_len, d_model))\n",
        "    for pos in range(max_seq_len):\n",
        "        for i in range(0, d_model, 2):\n",
        "            # This is the original implementation from the initial transformers paper\n",
        "            pos_enc[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "            pos_enc[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "    return torch.Tensor(pos_enc)\n",
        "\n",
        "# The hardcoded positional encoding matrix\n",
        "positional_encoding_matrix = get_positional_encoding(MAXIMUM_SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
        "\n",
        "print(\"The positional encoding matrix that we add to the input vectors looks as follows:\")\n",
        "plt.imshow(positional_encoding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-za1bgiIXodO"
      },
      "source": [
        "## 2.2 Transformer block\n",
        "\n",
        "Now, let's tackle the main part of the deep learning model: the transformer block. Transformers were invented for language translation and thus had encoding blocks and decoding blocks. GPT only uses decoding based blocks.\n",
        "\n",
        "The decoder block consists of 2 core components, masked self-attention and a feedforward neural network. Attention is a core concept of many of these nowadays super successful models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Multi-head masked self-attention\n",
        "\n",
        "This title makes it seem very difficult to understand. But it's actually a relatively simple concept. Let's tackle the different parts one by one.\n",
        "\n",
        "### Attention\n",
        "First, we look at attention. Here, I gladly follow Prof. Ryan Cotterell's explanation he gave in a lecture at ETH Zurich:\n",
        "\n",
        "![Attention example by Ryan Cotterell](https://raw.githubusercontent.com/serced/gpt2/main/assets/cotterell_attention_example.png)\n",
        "*Simple attention example by Prof. Cotterell (ETH Zurich). Source: Lecture 11, slide 30 (https://rycolab.io/classes/intro-nlp-f23/)*\n",
        "\n",
        "In the example table for values `V`, you have 4 rows and 3 columns. The rows represent the number of vectors and the columns represent the different dimensions of the vectors. If you are interested to retrieve the vector from the 3rd row, you can simply multiply a vector filled with zeros and a 1 at the 3rd dimension. With this, you retrieve the row-vector from the 3rd row!\n",
        "\n",
        "Attention generalizes this concept and instead of setting a specific index to 1, we distribute the 1 over all dimensions in $\\alpha$ . We might take 10% from the first and 90% of the third row vectors. This allows the vector to incorporate knowledge from it's context (the first vector also contributes 10% to the third vector's new representation).\n",
        "\n",
        "To compute attention scores (the $\\alpha$ vector), we need queries (what we are looking for), keys (what is available). We multiply them with the values (the data we want incrorporate context into).\n",
        "\n",
        "If you'd like to understand the attention mechanism more in-depth, check out Prof. Cotterell's lecture 11 starting at slide 25!\n",
        "\n",
        "\n",
        "### Self-attention\n",
        "The \"self\" in \"self-attention\" is just there because the input to compute the new contextualized vectors is always the same for the queries, keys, and values. We simply multiply or project the same input with different learned parameters to get keys, queries, and values. Then, we compute the attention scores using queries and keys, and do the values mixture as described above.\n",
        "\n",
        "\n",
        "### Multi-head\n",
        "When transformers as model architecture were introduced, they basically started learning the percentages to assign for different vectors to build the new vector representation of a token that incorporates the previous context (tokens). Moreover, instead of a `single` attention \"head\" like in the example before, they used multiple such \"heads\". The hypothesis was, that this allows the different heads to attend to different relevant information. For instance in a company description, one head might attend to adjectives that describe a company, whereas another one might attend to what the company does. Both heads process relevant but different information concurrently.\n",
        "\n",
        "\n",
        "### Masking\n",
        "What is masking in this context? The generative pretrained transformer keeps predicting the next token (character in our case). To teach this, we give as input a number sequence, but:\n",
        "- to predict the second character, it should only have access to the first character,\n",
        "- to predict the third character, it should only have access to the first two characters,\n",
        "- to predict the fourth character, it should only have access to the first three characters,\n",
        "\n",
        "... you get the idea. Since transformers can train with a text chunk at once (effectively, this means multiple training examples in one forward pass), you need to ensure that the attention can only span the previously seen tokens. Hence, the masking.\n",
        "\n",
        "\n",
        "\n",
        "## General notes to understand the code\n",
        "Now you should understand the next codeblock defining the `MultiHeadMaskedSelfAttention` code module! Well done! You can mainly look at the `forward(x)` function and understand what's going on there.\n",
        "\n",
        "1. **Note**: Clarifications on the `forward pass`:\n",
        "- `batch_size` means how many text chunks we pass per training prediction step. We use many examples at once, not just the many character prediction examples from the single text chunk! Talk about efficiency :)\n",
        "- `sequence_length` is the previously mentioned `MAXIMUM_SEQUENCE_LENGTH` that defines how long the character sequence is that our model should be able to handle\n",
        "- `embedding_size` is the `EMBEDDING_SIZE` that defines the number of vector dimensions that represent a single character\n",
        "\n",
        "2. **Note**: We split the character representations after the linear projections into multiple heads by rearranging the matrix from size `(batch_size, sequence_length, embedding_size)` to shape `(batch_size, number_of_heads, sequence_length, head_size)` and then do the multi-head attention computation before combining (concatenating) the sub-representations again.\n",
        "\n",
        "3. **Note**: The basic attention formula is\n",
        "$attended\\_values = softmax \\left(\\frac{Q K^T}\n",
        "{\\sqrt{embedding\\_size\\_keys}} \\right)* V$. $Q$ and $K$ are the queries and keys matrices. Before the softmax function, we need to mask the $QK^T$ result such that the attention scores of a current token cannot pay attention to future tokens of the same sequence. To do this, we build an upper triangular matrix that contains\n",
        "$-infinity$ (called `autoregressive_attention_mask` in the code). It looks as follows:\n",
        "        [[0, -inf, -inf, -inf],\n",
        "         [0,    0, -inf, -inf],\n",
        "         [0,    0,    0, -inf],\n",
        "         [0,    0,    0,    0]]\n",
        "When we add it to our score matrix and it is passed through the softmax function, this leads to a probability of $0$ where $-inf$ is.\n",
        "\n",
        "4. **Note** `F.softmax` is a function that is used to normalize output such that summing over the given output dimension adds up to 1. The attention scores need to add up to 1 to represent how much of each value vector should be incorporated into the new contextualized vector representation."
      ],
      "metadata": {
        "id": "Yx6hs_7BIy3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIHDADyGWAZb"
      },
      "outputs": [],
      "source": [
        "class MultiHeadMaskedSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    When we create a MultiHeadMaskedSelfAttention module,\n",
        "    we need to pass the arguments in the `__init__`. These are:\n",
        "\n",
        "    embedding_size: the vector dimensions\n",
        "    number_of_heads: the number of attention heads, i.e., the \"multi\" number\n",
        "    bias and dropout can be ignored for now\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, number_of_heads, bias, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.bias = bias\n",
        "        self.number_of_heads = number_of_heads\n",
        "        assert (\n",
        "            embedding_size % number_of_heads == 0\n",
        "        ), \"Embedding dimension has to be divisible by the number of heads\"\n",
        "\n",
        "        # A simple linear transformation that projects the input into three different spaces\n",
        "        self.queries_projection = nn.Linear(embedding_size, embedding_size)\n",
        "        self.keys_projection = nn.Linear(embedding_size, embedding_size)\n",
        "        self.values_projection = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_projection = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # See note 1\n",
        "        # Input (x) shape is (batch_size, sequence_length, embedding_size)\n",
        "        batch_size, sequence_length, embedding_size = x.size()\n",
        "        queries = self.queries_projection(x)\n",
        "        keys = self.keys_projection(x)\n",
        "        values = self.values_projection(x)\n",
        "\n",
        "        # See note 2\n",
        "        head_size = embedding_size // self.number_of_heads\n",
        "        queries = queries.view(\n",
        "            batch_size, sequence_length, self.number_of_heads, head_size\n",
        "        ).transpose(1, 2)\n",
        "        keys = keys.view(\n",
        "            batch_size, sequence_length, self.number_of_heads, head_size\n",
        "        ).transpose(1, 2)\n",
        "        values = values.view(\n",
        "            batch_size, sequence_length, self.number_of_heads, head_size\n",
        "        ).transpose(1, 2)\n",
        "\n",
        "        # See note 3\n",
        "        mask = torch.triu(\n",
        "            torch.ones((sequence_length, sequence_length), device=x.device),\n",
        "            diagonal=1,\n",
        "        )  # Strangely diagonal=1 means the diagonal is 0\n",
        "        autoregressive_attention_mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
        "\n",
        "        attention_scores = torch.matmul(queries, keys.transpose(-1, -2)) * (\n",
        "            1.0 / np.sqrt(keys.size(-1))\n",
        "        )\n",
        "        masked_attention_scores = attention_scores + autoregressive_attention_mask\n",
        "\n",
        "        # Compute self-attention scores. For F.softmax see note 4\n",
        "        masked_attention_scores = F.softmax(masked_attention_scores, dim=-1)\n",
        "        masked_attention_scores = self.dropout(masked_attention_scores)\n",
        "\n",
        "        # Shape: (batch_size, number_of_heads, sequence_length, head_size)\n",
        "        attended_values = torch.matmul(masked_attention_scores, values)\n",
        "\n",
        "        # Concatenate the heads\n",
        "        # Shape: (batch_size, sequence_length, embedding_size)\n",
        "        # .transpose(1, 2) changes the shape order to (batch_size, sequence_length, number_of_heads, head_size)\n",
        "        attended_values = (\n",
        "            attended_values.transpose(1, 2)\n",
        "            .contiguous()\n",
        "            .view(batch_size, sequence_length, embedding_size)\n",
        "        )\n",
        "\n",
        "        # Pass through a linear layer to get the final output\n",
        "        output = self.output_projection(attended_values)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Multi-layer perceptron (MLP)\n",
        "\n",
        "The second core module in the transformer decoder block is the MLP. It's made up of a learned linear projection, followed by a non-linearity and another learned linear projection. In deep-learning, non-linear activation functions are required such that neural networks can learn non-linear decision boundaries beyond what linear operations can. Without them, if one were to stack multiple linear projections, this would just end up in another linear projection and could be represented in one step. `gelu` stands for \"Gaussian Error Linear Unit\" which is just the non-linearity function (`activation function`) that was used in GPT2."
      ],
      "metadata": {
        "id": "7PNNII_PI54y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "    \"\"\"\n",
        "    input_size: the dimensions of the input\n",
        "    hidden_size: how wide the neural network layer should be\n",
        "    output_size: the dimensions you want the output to have\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.input_linear_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_linear_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_linear_layer(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.output_linear_layer(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JRmGI44jI6ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, now we can almost write the code for the transformer decoder block!\n",
        "We are missing the layer normalization, dropout, and residual connections.\n",
        "\n",
        "### 2.2.3 Layer normalization, dropout, and residual connections\n",
        "\n",
        "**Layer normalization** adjusts the data inside a neural network's layer by making sure all the inputs to the next layer have a similar scale, helping the network learn faster and more effectively. We use pytorch's provided `nn.LayerNorm` module.\n",
        "\n",
        "**Dropout** is a regularization technique which is only applied during training. It randomly turns off e.g. 20% of neurons/parameters in the layer to make the whole network more robust and prevent overfitting (when the network perfectly remembers training data, i.e., a lookup table). Again, we can use pytorch's provided `nn.Dropout` module.\n",
        "\n",
        "On the architecture picture we also see arrows that go from inbetween layers modules to a `+` sign. These are so-called **residual connections** and simply take the same input and add it back later on."
      ],
      "metadata": {
        "id": "TqUoZpSLMMqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZYnERJix_i4"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Take a look at the architecture image and you will notice that we now define\n",
        "    all required parts for the transformer block in our `__init__` function and\n",
        "    then use them in our `forward pass` on the input x.\n",
        "\n",
        "\n",
        "    embedding_size: the vector dimensions per character\n",
        "    n_attention_heads: the number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, n_attention_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_layer_normalization = nn.LayerNorm(embedding_size)\n",
        "\n",
        "        # Here we use our own MultiHeadMaskedSelfAttention class\n",
        "        self.masked_attention = MultiHeadMaskedSelfAttention(\n",
        "            embedding_size,\n",
        "            n_attention_heads,\n",
        "            bias=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.post_attention_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.pre_mlp_layer_normalization = nn.LayerNorm(embedding_size)\n",
        "        # Here we use our own MultiLayerPerceptron class\n",
        "        self.multi_layer_perceptron = MultiLayerPerceptron(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=embedding_size * 4,\n",
        "            output_size=embedding_size,\n",
        "        )\n",
        "        self.post_mlp_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape is (batch_size, sequence_length, embedding_dimension)\n",
        "        # First residual connection wrapper\n",
        "        residual = x.clone()\n",
        "        x = self.input_layer_normalization(x)\n",
        "        x = self.masked_attention(x)\n",
        "        x = self.post_attention_dropout(x)\n",
        "        x = x + residual\n",
        "\n",
        "        # Second residual connection wrapper\n",
        "        residual = x.clone()\n",
        "        x = self.pre_mlp_layer_normalization(x)\n",
        "        x = self.multi_layer_perceptron(x)\n",
        "        x = self.post_mlp_dropout(x)\n",
        "        x = x + residual\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome, we have the decoder transformer block together! Now we only need to define the full neural network. I also added a `generate` function to use it when the model is trained to predict the next character from any given input!"
      ],
      "metadata": {
        "id": "hAZNNfE4OejF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw-nLJgV-uwr"
      },
      "outputs": [],
      "source": [
        "class GenerativePretrainedTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    embedding_dimension: the number of vector dimensions that represent a single character\n",
        "    sequence_length: how many characters the model can process at most\n",
        "    n_transformer_blocks: the number of transformer decoder blocks used\n",
        "    n_attention_heads: the number of attention heads in each decoder block\n",
        "    vocabulary_size: the number of unique characters in our dataset\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dimension,\n",
        "        maximum_sequence_length,\n",
        "        n_transformer_blocks,\n",
        "        n_attention_heads,\n",
        "        vocabulary_size,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # A matrix mapping characters to vectors and the static positional encoding matrix\n",
        "        self.character_to_embedding_map = nn.Embedding(\n",
        "            vocabulary_size, embedding_dimension\n",
        "        )\n",
        "        self.positional_encoding = get_positional_encoding(\n",
        "            maximum_sequence_length, embedding_dimension\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Define the transformer decoder blocks used\n",
        "        self.transformer_decoder_blocks = nn.ModuleList([\n",
        "            TransformerDecoderBlock(\n",
        "                embedding_dimension, n_attention_heads, dropout=dropout\n",
        "            )\n",
        "            for _ in range(n_transformer_blocks)\n",
        "        ])\n",
        "\n",
        "        # The normalization layer and the layer used for predicting which character comes next\n",
        "        self.layer_normalization = nn.LayerNorm(embedding_dimension)\n",
        "        self.character_prediction_layer = nn.Linear(\n",
        "            embedding_dimension, vocabulary_size, bias=False\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, batch_of_character_sequences):\n",
        "        # Input shape: (batch_size, sequence_length)\n",
        "        x = self.character_to_embedding_map(batch_of_character_sequences)\n",
        "        character_positions_in_sequence = torch.arange(x.size(1))\n",
        "\n",
        "        x = x + self.positional_encoding[character_positions_in_sequence]\n",
        "\n",
        "        # Pass through all Transformer Decoder Blocks\n",
        "        # x shape is (batch_size, sequence_length, embedding_dimension)\n",
        "        for decoder_block in self.transformer_decoder_blocks:\n",
        "            x = decoder_block(x)\n",
        "\n",
        "        x = self.layer_normalization(x)\n",
        "        # These are called logits because you need to normalize them with F.softmax\n",
        "        # when actually generating text (see the `generate` function for this)\n",
        "        output_logits = self.character_prediction_layer(x)\n",
        "        return output_logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_text_indices, max_length=300, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        The generate function is used after training and we can use it to\n",
        "        predict the next characters from any input sequence.\n",
        "        \"\"\"\n",
        "        input_text_indices = input_text_indices.unsqueeze(0)\n",
        "        full_sequence = input_text_indices.clone()\n",
        "        current_sequence_length = input_text_indices.size(1)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Check that the context is not too long, otherwise cut it\n",
        "            if current_sequence_length > MAXIMUM_SEQUENCE_LENGTH:\n",
        "                input_text_indices = input_text_indices[:, -MAXIMUM_SEQUENCE_LENGTH:]\n",
        "\n",
        "            # Pass through the model\n",
        "            logits = self.forward(input_text_indices)\n",
        "            # We only need the logits for the last character\n",
        "            next_character_logits = logits[0, -1, :] / temperature\n",
        "            # Apply top-k sampling if needed\n",
        "            if top_k is not None:\n",
        "                top_k_characters, top_k_indices = torch.topk(next_character_logits)\n",
        "                # Set all logits to -infinity that are not in the top-k\n",
        "                # Effectively setting the probability to 0 after the softmax\n",
        "                next_character_logits[~top_k_indices] = float(\"-inf\")\n",
        "\n",
        "            # Transform the model prediction into a probability distribution and sample from it\n",
        "            next_character_probabilities = F.softmax(next_character_logits, dim=-1)\n",
        "            next_character = torch.multinomial(next_character_probabilities, 1)\n",
        "\n",
        "            # Append to the sequence\n",
        "            input_text_indices = torch.cat([input_text_indices, next_character.unsqueeze(0)], dim=1)\n",
        "            full_sequence = torch.cat([full_sequence, next_character.unsqueeze(0)], dim=1)\n",
        "            current_sequence_length += 1\n",
        "\n",
        "        return full_sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3&nbsp;Defining hyperparameters and creating the model\n",
        "\n",
        "Now we simply need to define the hyperparamters of our model. These are things like:\n",
        "- how long the sequences it should be able to process are,\n",
        "- what the vector dimensions for each character and the hidden representations are,\n",
        "- how many transformer decoder blocks we want, etc.\n",
        "\n",
        "and depend on various things like the amount of available computing power, data, etc."
      ],
      "metadata": {
        "id": "WQEl6lJEQj2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3yrq7pJOeWH"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_SIZE = 128                # the vector dimensions\n",
        "N_TRANSFORMER_DECODER_BLOCKS = 4    # how many decoder blocks\n",
        "N_ATTENTION_HEADS = 4               # how many attention heads per decoder block\n",
        "MAXIMUM_SEQUENCE_LENGTH = 128       # how many characters the model should be able to handle\n",
        "DROPOUT = 0.25                      # regularization paramter to prevent overfitting\n",
        "\n",
        "BATCH_SIZE = 64                     # how many sequences we pass in at once for training\n",
        "MAX_ITERATIONS = 2000               # how many gradient descent steps we want to do at most\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'     # Whether to use the GPU or CPU\n",
        "\n",
        "compile = False\n",
        "\n",
        "\n",
        "torch.device(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets create yourGPT!"
      ],
      "metadata": {
        "id": "NOx7FHgKRZEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_own_gpt = GenerativePretrainedTransformer(\n",
        "  embedding_dimension=EMBEDDING_SIZE,\n",
        "  n_attention_heads=N_ATTENTION_HEADS,\n",
        "  maximum_sequence_length=MAXIMUM_SEQUENCE_LENGTH,\n",
        "  n_transformer_blocks=N_TRANSFORMER_DECODER_BLOCKS,\n",
        "  vocabulary_size=n_unique_characters,\n",
        ").to(DEVICE)\n",
        "\n",
        "if compile:\n",
        "    our_own_gpt = torch.compile(our_own_gpt)"
      ],
      "metadata": {
        "id": "7AdRwfQwRaJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4&nbsp;Defining the training loop\n",
        "\n",
        "Finally, to train our model we need to define the training loop. Here I am making use of further utilities provided by PyTorch that make it easier to handle training data, etc. This is less important to understand the mechanics of chatGPT.\n",
        "\n",
        "By now, you should understand that we pass in multiple character sequences that are represented as numbers, and the model learns to predict the next character from all previous characters from all sequences at once! This is why we can train transformers efficiently since we can make use of matrices and graphic processing units (GPUs) to parallelize many training samples.\n",
        "\n",
        "Missing pieces for training are:\n",
        "1. A loss function or `criterion` that defines the difference between what our model predicted and what is actually true.\n",
        "\n",
        "2. An optimizer that that manipulates `knobs` based on how much one knob influenced the prediction and thereby the loss.\n",
        "\n",
        "3. A training dataset to train the model and a validation dataset to check whether the model is overfitting (meaning it's learning the training data by heart) based on the TinyShakespeare dataset that we encoded as number sequence."
      ],
      "metadata": {
        "id": "iSdCjrITRmRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and the loss function\n",
        "optimizer = torch.optim.AdamW(our_own_gpt.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# First we need to encode the training and validation data\n",
        "# tensors are a generalization of 1-dimensional vectors, 2-dimensional matrices, to any n-dimensions\n",
        "encoded_train_data = torch.tensor(tokenizer.encode(train_data), dtype=torch.long)\n",
        "encoded_validation_data = torch.tensor(tokenizer.encode(validation_data), dtype=torch.long)\n",
        "\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, sequence_length):\n",
        "        self.data = data.to(DEVICE)\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        This function defines what are training inputs (x) and the targets (y).\n",
        "        For instance if our data is [0, 2, 30, 21], the input x is [0, 2, 30],\n",
        "        and the target y is [2, 30, 21]\n",
        "        \"\"\"\n",
        "        x = self.data[idx : idx + self.sequence_length]\n",
        "        y = self.data[idx + 1 : idx + self.sequence_length + 1]\n",
        "        return x, y\n",
        "\n",
        "train_dataset = TextDataset(encoded_train_data, MAXIMUM_SEQUENCE_LENGTH)\n",
        "validation_dataset = TextDataset(encoded_validation_data, MAXIMUM_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "# We can then define a DataLoader. This is a utility class to sample multiple sequences\n",
        "# and whether we want to randomize the order of the batch sequence or not\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    dataset=validation_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "adEbmG1lURXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we get to the training!"
      ],
      "metadata": {
        "id": "QG0csLJBUyRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SNk4xV8Ow9j"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "# Adding simple way to reload best model based on validation loss\n",
        "best_validation_loss = float(\"inf\")\n",
        "# Adding scaler to make training faster\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=DEVICE=='cuda')\n",
        "iteration = 0\n",
        "validation_interval = 100\n",
        "log_interval = 20\n",
        "\n",
        "\n",
        "# Now we can start the training loop\n",
        "while iteration < MAX_ITERATIONS:\n",
        "    our_own_gpt.train()\n",
        "    for x, y in train_loader:\n",
        "        # stop if we hit the max iteration count\n",
        "        if iteration >= MAX_ITERATIONS:\n",
        "            break\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Speedup trick\n",
        "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
        "            # Forward pass\n",
        "            logits = our_own_gpt(x)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "\n",
        "\n",
        "        # Backward pass to update gradients of all parameters based on the loss\n",
        "        # signaling how they should be adjusted to minimize the loss\n",
        "        scaler.scale(loss).backward()\n",
        "        # Update the weights/parameters accordingly\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        if iteration % log_interval == 0 and not iteration % validation_interval == 0:\n",
        "            print(f\"Iteration: {iteration} - Training loss: {loss.item()}\")\n",
        "\n",
        "        if iteration % validation_interval == 0 or iteration == MAX_ITERATIONS:\n",
        "\n",
        "            # At the end of each epoch we check how our model performs on the validation set\n",
        "            our_own_gpt.eval()\n",
        "            with torch.no_grad():\n",
        "                validation_loss = 0\n",
        "                for x, y in validation_loader:\n",
        "                    logits = our_own_gpt(x)\n",
        "                    validation_loss += criterion(logits.view(-1, logits.size(-1)), y.view(-1)).item()\n",
        "                validation_loss /= len(validation_loader)\n",
        "\n",
        "            # If the model performs better, we save the learned parameters/weights\n",
        "            if validation_loss < best_validation_loss:\n",
        "                best_validation_loss = validation_loss\n",
        "                torch.save(our_own_gpt.state_dict(), 'best_model.pth')\n",
        "\n",
        "\n",
        "            print(f\"Iteration: {iteration} - Training loss: {loss.item()} - Validation loss: {validation_loss}\")\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "\n",
        "now = time.time()\n",
        "print(f\"iteration time: {now - start}\")\n",
        "\n",
        "# Reload best model weights/parameters\n",
        "state_dict = torch.load(\"best_model.pth\")\n",
        "our_own_gpt.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How big is our model? Well compared to chatGPT it's extremly small. The model from OpenAI has about ~1.8 trillion parameters, meaning 1,800 billion parameters, or 1,800,000 million parameters. Ours in comparsion has:"
      ],
      "metadata": {
        "id": "dJ6w4avFWLfj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmstK7RCKoGu"
      },
      "outputs": [],
      "source": [
        "total_params = sum(p.numel() for p in our_own_gpt.parameters())\n",
        "print(f\"This model has {total_params/1e3} thousand parameters. Thus, it is incredibly small compared to chatGPT.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5&nbsp;Generating text with yourGPT\n",
        "And now, let's see if our model is actually capabable of generating some Shakespeare-ish looking text:"
      ],
      "metadata": {
        "id": "o2NG2JUvWojH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns573WjcLGRj"
      },
      "outputs": [],
      "source": [
        "# Play around with the input string to see what you can generate!\n",
        "input_string = \"Oh there goeth my\"\n",
        "\n",
        "encoded_input = tokenizer.encode(input_string)\n",
        "input_text_indices = torch.tensor(encoded_input, dtype=torch.long).to(DEVICE)\n",
        "\n",
        "output_character_indices = our_own_gpt.generate(input_text_indices, max_length=100)\n",
        "\n",
        "decoded_output = tokenizer.decode(output_character_indices.squeeze(0).cpu().tolist())\n",
        "print(decoded_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.&nbsp;Using a larger trained model\n",
        "\n",
        "I also trained a larger model besides what we defined in this notebook. It is not much better as it's not perfectly tuned, though it's fun to see Shakespeare-looking gibberish :)\n",
        "\n",
        "Let's load it up to play around. Can you get better text out of this one than the tiny one you trained?"
      ],
      "metadata": {
        "id": "HigCU6sAUSsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the larger trained model from remote. Run this cell, ignore the code.\n",
        "# When I trained the larger model, I also used torch.compile\n",
        "# This is a function to undo this on the saved model state\n",
        "%%capture\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login(anonymous=\"must\")\n",
        "\n",
        "api = wandb.Api()\n",
        "artifact = api.artifact('serced/myGPT/large_model:latest', type='model')\n",
        "large_model_checkpoint_path = artifact.download()\n",
        "\n",
        "def remove_compile_prefix(state_dict):\n",
        "    new_state_dict = {}\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('_orig_mod.'):\n",
        "            new_key = k[len('_orig_mod.'):]\n",
        "            new_state_dict[new_key] = v\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "    return new_state_dict"
      ],
      "metadata": {
        "id": "0seYgQLhh4Eb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to recreate the larger model architecture as I trained it. For this, we need to use the same hyperparameters and create a new `our_own_gpt_large` object with them."
      ],
      "metadata": {
        "id": "jqfRqg5lm5-G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-RfWwyvUHom"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_SIZE = 384                # the vector dimensions\n",
        "N_TRANSFORMER_DECODER_BLOCKS = 6    # how many decoder blocks\n",
        "N_ATTENTION_HEADS = 6               # how many attention heads per decoder block\n",
        "MAXIMUM_SEQUENCE_LENGTH = 256       # how many characters the model should be able to handle\n",
        "\n",
        "torch.device(DEVICE)\n",
        "\n",
        "\n",
        "# Create a new larger model\n",
        "our_own_gpt_large = GenerativePretrainedTransformer(\n",
        "  embedding_dimension=EMBEDDING_SIZE,\n",
        "  n_attention_heads=N_ATTENTION_HEADS,\n",
        "  maximum_sequence_length=MAXIMUM_SEQUENCE_LENGTH,\n",
        "  n_transformer_blocks=N_TRANSFORMER_DECODER_BLOCKS,\n",
        "  vocabulary_size=n_unique_characters,\n",
        ").to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the learned parameters from the fetched remote model\n",
        "state_dict_larger_model = torch.load(large_model_checkpoint_path + '/best_model.pth', map_location=torch.device(DEVICE))\n",
        "state_dict_larger_model = remove_compile_prefix(state_dict_larger_model)\n",
        "our_own_gpt_large.load_state_dict(state_dict_larger_model)"
      ],
      "metadata": {
        "id": "h8tb7cRTbcYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how big this larger trained model is."
      ],
      "metadata": {
        "id": "CsD3mz0zhOoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_large = sum(p.numel() for p in our_own_gpt_large.parameters())\n",
        "print(f\"This model has {total_params_large/1e6:.2f} million parameters. It's already {total_params_large/total_params:.1f} times bigger than your model. \\nHowever, it is still incredibly small compared to chatGPT.\")"
      ],
      "metadata": {
        "id": "GDq8HDmWdYmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltfDjIT2UwGm"
      },
      "outputs": [],
      "source": [
        "# Play around with the input string to see what you can generate!\n",
        "input_string = \"Oh there goeth my\"\n",
        "\n",
        "encoded_input = tokenizer.encode(input_string)\n",
        "input_text_indices = torch.tensor(encoded_input).to(DEVICE)\n",
        "\n",
        "output_character_indices = our_own_gpt_large.generate(input_text_indices, max_length=500, temperature=0.8)\n",
        "decoded_output = tokenizer.decode(output_character_indices.squeeze(0).cpu().tolist())\n",
        "print(decoded_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using GPT2 from OpenAI\n",
        "\n",
        "Let's check a larger trained model which we can easily access from the internet. We are using OpenAI's GPT2. To do this, we simply need to get the tokenizer that maps text to numbers and back, as well as the model. It's very little code!"
      ],
      "metadata": {
        "id": "LWTzmxXEOycB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# install the huggingface transformers library that gives access to millions of trained models\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EupPwoSLPzHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Import the tokenizer and language model scaffolds\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the tokenizer that maps text to numbers and the inverse\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Load the trained model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "nAbbaeEwPLfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check this model's size."
      ],
      "metadata": {
        "id": "s0PN030ZaBWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\"\"This model has {model.num_parameters()/1e6:.2f} million parameters.\\nIt's already {model.num_parameters()/total_params:.1f} times bigger than your model. \\nHowever, it is still incredibly small compared to chatGPT.\\nRemember, chatGPT has about ~1.8 trillion parameters (though, likely not all of them are used in one prediction step).\"\"\")"
      ],
      "metadata": {
        "id": "BZIPLzY9aDpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input text\n",
        "input_text = \"Are you conscious?\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text. The tokens are more than just single characters, hence, we need less\n",
        "# for the same text length\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=100,\n",
        "    do_sample=True,\n",
        "    top_k=40,\n",
        "    temperature=1,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode the generated tokens\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "KJEn7eHARaqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional information\n",
        "\n",
        "## How does chatGPT deal with image data and audio data?\n",
        "\n",
        "These powerful models all rely on the transformer architecture, specifically, the decoder blocks which we implemented from scratch. That means, we need a way to adapt images into series of tokens. Similarly, audio data can be interpreted as tokens as well.\n",
        "\n",
        "For images, we cut the image into small squares and flatten the squares into a vector by using a linear projection on the square and then flatten this into a the vector representation. Hence, as an example we have an image of size $(16, 16)$, we can cut this up into 4 squares of the size $(4, 4)$. We then flatten each of these squares into a vector of size $4 * 4 = 16$. Starting at the top-left we can thus transformed the image of size $(16,16)$ int a series of 4 vectors of size $16$ where each vector is again a token, just like our character representation (the `nn.Embedding` of the character). This results in the image being represented as the sequence of four tokens: $(4, 16)$. If you want to understand this better, I recommend having a look at this post from [Dennis Turp called \"A Visual Guide to Vision Transformers\"](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html).\n",
        "\n",
        "Audio data can also be represented as images (spectograms) and then you process them just like images.\n",
        "\n",
        "Thus, in the end, you can tokenize text, images, and audio that you can pass through large transformer models.\n",
        "\n",
        "## I would like to learn more\n",
        "\n",
        "Check out these links which previously served as resources for me and are great to learn more about LLMs.\n",
        "\n",
        "- [nanoGPT by Andrej Karpathy (ex-OpenAI)](https://github.com/karpathy/nanoGPT/). Awesome code repository with which you can train your own GPT models, though, for beginners there is already a lot of little speedup tricks, etc. which makes it harder to understand. It has a lot of comments though, hence, it still is possible to follow it. He also created a great [video](https://www.youtube.com/watch?v=kCc8FmEb1nY) available on Youtube that is well structured.\n",
        "- [The annotated transformer by Sasha Rush et al.](https://nlp.seas.harvard.edu/annotated-transformer/). This is an excellent blog post explaining the original transformer architecture when it was introduced.\n",
        "- [The annotated GPT-2 from Aman Arora](https://amaarora.github.io/posts/2020-02-18-annotatedGPT2.html). Great blogpost with the same principle on GPT-2.\n",
        "- [Prof. Ryan Cotterell's Natural Language Processing course slides](https://rycolab.io/classes/intro-nlp-f23/). Great slides to understand the history of NLP and the attention mechanism/transformer (see Lecture 11).\n",
        "- [Vision Transformer from Scratch from Brian Pulver](https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c). A great blogpost that implements a vision transformer from scratch. First, briefly scroll through the mentioned [\"Visual Guide to Vision Transformers\"](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html) to get a rough idea how the end architecture looks like and then read Brian's post.\n",
        "\n",
        "If you would like to learn more or are generally interested, reach out on [LinkedIn](https://www.linkedin.com/in/severinhusmann/). I'd be happy to chat."
      ],
      "metadata": {
        "id": "LMZfYXXtwrW7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzQ2bDBq4Myg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}